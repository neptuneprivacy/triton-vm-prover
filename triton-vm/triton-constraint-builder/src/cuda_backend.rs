use std::collections::HashMap;
use std::fmt::Write as _;

use constraint_circuit::BinOp;
use constraint_circuit::CircuitExpression;
use constraint_circuit::ConstraintCircuit;
use constraint_circuit::InputIndicator;

use crate::Constraints;

pub struct CudaBackend {
    /// Cache of already-generated subexpressions to avoid duplication
    expr_cache: HashMap<usize, usize>,  // circuit_id -> stack_slot
}

impl Default for CudaBackend {
    fn default() -> Self {
        Self {
            expr_cache: HashMap::new(),
        }
    }
}

// Bytecode opcodes (must match bytecode_interpreter.cuh)
#[repr(u8)]
#[derive(Debug, Clone, Copy)]
enum BytecodeOp {
    LoadMainRow = 0,
    LoadAuxRow = 1,
    LoadChallenge = 2,
    LoadNextMainRow = 3,
    LoadNextAuxRow = 4,
    LoadConstXField = 5,
    LoadConstBField = 6,
    XFieldAdd = 10,
    XFieldMul = 11,
    Dup = 15,  // Duplicate top of stack
    StoreOutput = 20,
}

impl CudaBackend {
    /// Generate complete CUDA constraint evaluation code with bytecode interpreter
    pub fn constraint_evaluation_code(constraints: &Constraints) -> String {
        let mut cuda_code = String::new();

        // Consume the constraint monads to get actual circuits
        let init_circuits = constraints.init();
        let cons_circuits = constraints.cons();
        let tran_circuits = constraints.tran();
        let term_circuits = constraints.term();

        // Generate header
        writeln!(&mut cuda_code, "// Auto-generated CUDA bytecode constraint evaluation").unwrap();
        writeln!(&mut cuda_code, "// DO NOT EDIT - Generated by CudaBackend").unwrap();
        writeln!(&mut cuda_code).unwrap();

        writeln!(&mut cuda_code, "#include \"field_arithmetic.cuh\"").unwrap();
        writeln!(&mut cuda_code, "#include \"bytecode_interpreter.cuh\"").unwrap();
        writeln!(&mut cuda_code).unwrap();

        // Generate constraint counts
        writeln!(&mut cuda_code, "// Constraint counts").unwrap();
        writeln!(&mut cuda_code, "#define NUM_INITIAL_CONSTRAINTS {}", init_circuits.len()).unwrap();
        writeln!(&mut cuda_code, "#define NUM_CONSISTENCY_CONSTRAINTS {}", cons_circuits.len()).unwrap();
        writeln!(&mut cuda_code, "#define NUM_TRANSITION_CONSTRAINTS {}", tran_circuits.len()).unwrap();
        writeln!(&mut cuda_code, "#define NUM_TERMINAL_CONSTRAINTS {}", term_circuits.len()).unwrap();
        writeln!(&mut cuda_code).unwrap();

        // Compile bytecode for each constraint type
        let init_bytecode = Self::compile_constraints(&init_circuits, false);
        let cons_bytecode = Self::compile_constraints(&cons_circuits, false);
        let tran_bytecode = Self::compile_constraints(&tran_circuits, true);
        let term_bytecode = Self::compile_constraints(&term_circuits, false);

        // Generate bytecode arrays
        Self::generate_bytecode_array(&mut cuda_code, "init_bytecode", &init_bytecode);
        Self::generate_bytecode_array(&mut cuda_code, "cons_bytecode", &cons_bytecode);
        Self::generate_bytecode_array(&mut cuda_code, "tran_bytecode", &tran_bytecode);
        Self::generate_bytecode_array(&mut cuda_code, "term_bytecode", &term_bytecode);

        // Generate wrapper functions that call the interpreter
        Self::generate_wrapper_function(
            &mut cuda_code,
            "evaluate_initial_constraints",
            "init_bytecode",
            init_bytecode.len(),
            init_circuits.len(),
            false
        );

        Self::generate_wrapper_function(
            &mut cuda_code,
            "evaluate_consistency_constraints",
            "cons_bytecode",
            cons_bytecode.len(),
            cons_circuits.len(),
            false
        );

        Self::generate_wrapper_function(
            &mut cuda_code,
            "evaluate_transition_constraints",
            "tran_bytecode",
            tran_bytecode.len(),
            tran_circuits.len(),
            true
        );

        Self::generate_wrapper_function(
            &mut cuda_code,
            "evaluate_terminal_constraints",
            "term_bytecode",
            term_bytecode.len(),
            term_circuits.len(),
            false
        );

        cuda_code
    }

    /// Compile a list of constraints into bytecode
    ///
    /// IMPORTANT: Must match RustBackend's ordering!
    /// RustBackend outputs: [main_constraints (BField, lifted)] + [aux_constraints (XField)]
    /// So we must partition and order the same way.
    fn compile_constraints<II: InputIndicator>(
        constraints: &[ConstraintCircuit<II>],
        has_next_row: bool
    ) -> Vec<u8> {
        let mut bytecode = Vec::new();
        let mut backend = Self::default();

        // Partition constraints the same way RustBackend does:
        // main_constraints: evaluate to BFieldElement
        // aux_constraints: evaluate to XFieldElement
        let (main_constraints, aux_constraints): (Vec<_>, Vec<_>) = constraints
            .iter()
            .partition(|constraint| constraint.evaluates_to_base_element());

        // Process in the same order as RustBackend: main first, then aux
        // This ensures output indices match between GPU and CPU
        let ordered_constraints = main_constraints.iter().chain(aux_constraints.iter());

        for (idx, constraint) in ordered_constraints.enumerate() {
            backend.expr_cache.clear();
            backend.compile_constraint(&mut bytecode, constraint, has_next_row);

            // Store result to output[idx]
            bytecode.push(BytecodeOp::StoreOutput as u8);
            bytecode.push((idx >> 8) as u8);  // High byte
            bytecode.push((idx & 0xFF) as u8); // Low byte
        }

        bytecode
    }

    /// Compile a single constraint circuit into bytecode
    fn compile_constraint<II: InputIndicator>(
        &mut self,
        bytecode: &mut Vec<u8>,
        circuit: &ConstraintCircuit<II>,
        has_next_row: bool
    ) {
        // DISABLED: DUP-based caching doesn't work because cached values aren't always on top of stack
        // Without caching, bytecode is larger but functionally correct
        // We use __device__ memory instead of __constant__ to bypass 64KB limit
        // if self.expr_cache.contains_key(&circuit.id) {
        //     bytecode.push(BytecodeOp::Dup as u8);
        //     return;
        // }

        match &circuit.expression {
            CircuitExpression::BConst(bfe) => {
                // Load BField constant (lifted to XField)
                bytecode.push(BytecodeOp::LoadConstBField as u8);
                let raw = bfe.raw_u64();
                bytecode.extend_from_slice(&raw.to_be_bytes());
            }

            CircuitExpression::XConst(xfe) => {
                // Load XField constant
                bytecode.push(BytecodeOp::LoadConstXField as u8);
                bytecode.extend_from_slice(&xfe.coefficients[0].raw_u64().to_be_bytes());
                bytecode.extend_from_slice(&xfe.coefficients[1].raw_u64().to_be_bytes());
                bytecode.extend_from_slice(&xfe.coefficients[2].raw_u64().to_be_bytes());
            }

            CircuitExpression::Input(input) => {
                let column = input.column();
                let is_current = input.is_current_row();
                let is_main = input.is_main_table_column();

                let opcode = match (is_current, is_main) {
                    (true, true) => BytecodeOp::LoadMainRow,
                    (true, false) => BytecodeOp::LoadAuxRow,
                    (false, true) => BytecodeOp::LoadNextMainRow,
                    (false, false) => BytecodeOp::LoadNextAuxRow,
                };

                bytecode.push(opcode as u8);
                bytecode.push((column >> 8) as u8);  // High byte
                bytecode.push((column & 0xFF) as u8); // Low byte
            }

            CircuitExpression::Challenge(idx) => {
                bytecode.push(BytecodeOp::LoadChallenge as u8);
                bytecode.push((*idx >> 8) as u8);  // High byte
                bytecode.push((*idx & 0xFF) as u8); // Low byte
            }

            CircuitExpression::BinOp(binop, lhs, rhs) => {
                // Recursively compile left and right operands
                self.compile_constraint(bytecode, &lhs.borrow(), has_next_row);
                self.compile_constraint(bytecode, &rhs.borrow(), has_next_row);

                // Generate operation
                let op = match binop {
                    BinOp::Add => BytecodeOp::XFieldAdd,
                    BinOp::Mul => BytecodeOp::XFieldMul,
                };
                bytecode.push(op as u8);
            }
        }

        // DISABLED: Not using cache - see comment above about DUP limitations
        // self.expr_cache.insert(circuit.id, 0);
    }

    /// Generate a C array for bytecode
    fn generate_bytecode_array(cuda_code: &mut String, name: &str, bytecode: &[u8]) {
        writeln!(cuda_code, "// Bytecode for {} ({} bytes)", name, bytecode.len()).unwrap();

        // Decode and print first few instructions for debugging
        if name == "BYTECODE_INITIAL" && !bytecode.is_empty() {
            eprintln!("  [Bytecode Debug] Decoding first initial constraint bytecode:");
            let mut pc = 0;
            let mut instr_count = 0;
            while pc < bytecode.len() && instr_count < 10 {
                let op = bytecode[pc];
                pc += 1;
                eprint!("    [{}] ", instr_count);
                match op {
                    0 => { // LOAD_MAIN_ROW
                        if pc + 1 < bytecode.len() {
                            let idx = ((bytecode[pc] as u16) << 8) | (bytecode[pc+1] as u16);
                            eprintln!("LOAD_MAIN_ROW[{}]", idx);
                            pc += 2;
                        }
                    }
                    1 => { // LOAD_AUX_ROW
                        if pc + 1 < bytecode.len() {
                            let idx = ((bytecode[pc] as u16) << 8) | (bytecode[pc+1] as u16);
                            eprintln!("LOAD_AUX_ROW[{}]", idx);
                            pc += 2;
                        }
                    }
                    2 => { // LOAD_CHALLENGE
                        if pc + 1 < bytecode.len() {
                            let idx = ((bytecode[pc] as u16) << 8) | (bytecode[pc+1] as u16);
                            eprintln!("LOAD_CHALLENGE[{}]", idx);
                            pc += 2;
                        }
                    }
                    5 => eprintln!("ADD"),
                    6 => eprintln!("SUB"),
                    7 => eprintln!("MUL"),
                    8 => eprintln!("NEG"),
                    9 => { // LOAD_CONSTANT
                        if pc + 23 < bytecode.len() {
                            eprintln!("LOAD_CONSTANT[24 bytes]");
                            pc += 24;
                        }
                    }
                    10 => { // STORE_OUTPUT
                        if pc + 1 < bytecode.len() {
                            let idx = ((bytecode[pc] as u16) << 8) | (bytecode[pc+1] as u16);
                            eprintln!("STORE_OUTPUT[{}]", idx);
                            pc += 2;
                            break; // Stop after first output
                        }
                    }
                    _ => eprintln!("UNKNOWN_OP[{}]", op),
                }
                instr_count += 1;
            }
        }

        // Use __device__ memory (global) to bypass 64KB __constant__ memory limit
        // Without CSE caching, bytecode is ~88KB which exceeds __constant__ limit
        writeln!(cuda_code, "__device__ uint8_t {}[] = {{", name).unwrap();

        // Output in rows of 16 bytes for readability
        for (i, chunk) in bytecode.chunks(16).enumerate() {
            write!(cuda_code, "   ").unwrap();
            for (j, byte) in chunk.iter().enumerate() {
                write!(cuda_code, " 0x{:02x}", byte).unwrap();
                if i * 16 + j < bytecode.len() - 1 {
                    write!(cuda_code, ",").unwrap();
                }
            }
            writeln!(cuda_code).unwrap();
        }

        writeln!(cuda_code, "}};").unwrap();
        writeln!(cuda_code).unwrap();
    }

    /// Generate wrapper function that calls interpreter
    fn generate_wrapper_function(
        cuda_code: &mut String,
        func_name: &str,
        bytecode_name: &str,
        bytecode_len: usize,
        output_count: usize,
        has_next_row: bool
    ) {
        writeln!(cuda_code, "// Wrapper function for {}", func_name).unwrap();
        writeln!(cuda_code, "__device__ void {}(", func_name).unwrap();
        writeln!(cuda_code, "    const BFieldElement* main_row,").unwrap();
        writeln!(cuda_code, "    const XFieldElement* aux_row,").unwrap();

        if has_next_row {
            writeln!(cuda_code, "    const BFieldElement* next_main_row,").unwrap();
            writeln!(cuda_code, "    const XFieldElement* next_aux_row,").unwrap();
        }

        writeln!(cuda_code, "    const XFieldElement* challenges,").unwrap();
        writeln!(cuda_code, "    XFieldElement* output)").unwrap();
        writeln!(cuda_code, "{{").unwrap();

        writeln!(cuda_code, "    interpret_constraints(").unwrap();
        writeln!(cuda_code, "        {},", bytecode_name).unwrap();
        writeln!(cuda_code, "        {},", bytecode_len).unwrap();
        writeln!(cuda_code, "        main_row,").unwrap();
        writeln!(cuda_code, "        aux_row,").unwrap();

        if has_next_row {
            writeln!(cuda_code, "        next_main_row,").unwrap();
            writeln!(cuda_code, "        next_aux_row,").unwrap();
        } else {
            writeln!(cuda_code, "        nullptr,").unwrap();
            writeln!(cuda_code, "        nullptr,").unwrap();
        }

        writeln!(cuda_code, "        challenges,").unwrap();
        writeln!(cuda_code, "        output,").unwrap();
        writeln!(cuda_code, "        {}", output_count).unwrap();
        writeln!(cuda_code, "    );").unwrap();
        writeln!(cuda_code, "}}").unwrap();
        writeln!(cuda_code).unwrap();
    }
}
